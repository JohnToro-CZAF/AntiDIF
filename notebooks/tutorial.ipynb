{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tutorial on running inference on custom inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First find the pdb ids of the prots of intrest\n",
    "\n",
    "Here we use pdbs from the sabdab test set used in AntiDIF / AntiFold/ ABMPNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "388"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ids = pd.read_csv('data/raw_data/test_pdb_ids', index_col=0)\n",
    "len(test_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now to create the custom csv file we need the dir path that are pdb files will be in\n",
    "pdb_file_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def find_inps_csv(pdb_ids, dir, all_chains=True):\n",
    "    resut = {}\n",
    "    resut['pdb_paths'] = [dir + iden + '.pdb' for iden in pdb_ids]\n",
    "    if all_chains:\n",
    "        resut['chains'] = ['all'] * len(pdb_ids)\n",
    "    else: \n",
    "        raise NotImplementedError('only all chains support so far')\n",
    "    return pd.DataFrame(resut)\n",
    "\n",
    "pdb_file_dir = '/Users/nik/Documents/tf_ox/code/inv_folding/data/sabdab/' #dir pdbs will be in\n",
    "\n",
    "sab_test_inps = find_inps_csv(test_ids['0'], pdb_file_dir)\n",
    "#sab_test_inps.to_csv()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then simply save the df and update the path in config.yaml for custom_pdb_input to your custom csv."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model inference \n",
    "\n",
    "And\n",
    "### Saving data as a pkl\n",
    "\n",
    "Processing the data from a pdb can be time consuming if you want to re-run inference multiple times using the same data, here we save the data as a pickle file so that it can be quickly loaded in during inference. \n",
    "\n",
    "once saved set load_pkl to fpath of pkl dataset to load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('pi-rldif-ft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.mod_pifold import InverseFoldingDiffusionPiFoldModel\n",
    "from data.dataset import RLDIFDataset\n",
    "from utils.utils import load_config\n",
    "from torch.utils.data import DataLoader\n",
    "import pickle as pkl\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/raw_data/example.csv'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args = load_config('./configs/config.yaml')\n",
    "master_config = load_config('./configs/master_config.yaml')\n",
    "\n",
    "args.data.docker = False\n",
    "args.data.rldif = args.rldif\n",
    "args.data.protein_mpnn = args.protein_mpnn\n",
    "\n",
    "args.data.custom_pdb_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 66.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entered 1 samples into Redis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#test (have to change config.yaml for this)\n",
    "custom_ds = RLDIFDataset(args.data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/raw_data/example_ds.pkl', 'wb') as f:\n",
    "    pkl.dump(custom_ds, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.data.load_pkl = 'data/raw_data/example_ds.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.data.load_pkl:\n",
    "    with open(args.data.load_pkl, 'rb') as f:\n",
    "        dataset = pkl.load(f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load model for inf\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load model and weights \n",
    "\n",
    "args.pifold_model.free_positions = args.free_positions\n",
    "model = InverseFoldingDiffusionPiFoldModel(args.pifold_model).to(device)\n",
    "m_path = args.m_name\n",
    "state_dict = torch.load(m_path, map_location=device)['state_dict'] #n.b added map_location \n",
    "new_state_dict = {}\n",
    "for k, v in state_dict.items():\n",
    "    new_state_dict[k.replace('model.', '')] = v\n",
    "model.load_state_dict(new_state_dict)\n",
    "\n",
    "collate_function = model.collate_fn\n",
    "dataloader = DataLoader(\n",
    "                dataset,\n",
    "                batch_size=1, \n",
    "                shuffle=False,\n",
    "                collate_fn=collate_function,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model for inf need to also load weights.\n",
    "for batch in dataloader:\n",
    "    out = model.sample(batch.clone().to(device), closure=True)\n",
    "    break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlfdif",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
